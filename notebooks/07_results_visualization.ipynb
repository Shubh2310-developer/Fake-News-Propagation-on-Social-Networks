{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69c50bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "FAKE NEWS GAME THEORY - RESULTS VISUALIZATION\n",
      "================================================================================\n",
      "\n",
      "Output directories:\n",
      "  - Figures: /home/ghost/fake-news-game-theory/data/results/figures\n",
      "  - Reports: /home/ghost/fake-news-game-theory/reports\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import networkx as nx\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pickle\n",
    "from typing import Dict, List, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "plt.rcParams['axes.labelsize'] = 12\n",
    "plt.rcParams['axes.titlesize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 10\n",
    "plt.rcParams['ytick.labelsize'] = 10\n",
    "\n",
    "# Define paths\n",
    "BASE_DIR = Path('/home/ghost/fake-news-game-theory')\n",
    "DATA_DIR = BASE_DIR / 'data'\n",
    "RESULTS_DIR = DATA_DIR / 'results'\n",
    "FIGURES_DIR = RESULTS_DIR / 'figures'\n",
    "REPORTS_DIR = BASE_DIR / 'reports'\n",
    "\n",
    "# Create output directories\n",
    "FIGURES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "REPORTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"FAKE NEWS GAME THEORY - RESULTS VISUALIZATION\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nOutput directories:\")\n",
    "print(f\"  - Figures: {FIGURES_DIR}\")\n",
    "print(f\"  - Reports: {REPORTS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2acfe427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "LOADING RESULTS FROM ALL NOTEBOOKS\n",
      "================================================================================\n",
      "✓ Loaded model comparison: 7 models\n",
      "✓ Loaded network analysis results\n",
      "✓ Loaded propagation results: 64 rows\n",
      "✓ Loaded game theory results: 24 rows\n",
      "✓ Loaded simulation results: 360 rows\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# SECTION 2: Load All Results\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"LOADING RESULTS FROM ALL NOTEBOOKS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "def load_results():\n",
    "    \"\"\"Load all results from previous notebooks\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # Load model comparison results (from notebook 03)\n",
    "    try:\n",
    "        model_comparison = pd.read_csv(RESULTS_DIR / 'model_comparison.csv')\n",
    "        results['model_comparison'] = model_comparison\n",
    "        print(f\"✓ Loaded model comparison: {len(model_comparison)} models\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"✗ Model comparison not found\")\n",
    "        results['model_comparison'] = None\n",
    "    \n",
    "    # Load network analysis results (from notebook 04)\n",
    "    try:\n",
    "        with open(RESULTS_DIR / 'enhanced_results_complete.json', 'r') as f:\n",
    "            network_results = json.load(f)\n",
    "        results['network_analysis'] = network_results\n",
    "        print(f\"✓ Loaded network analysis results\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"✗ Network analysis results not found\")\n",
    "        results['network_analysis'] = None\n",
    "    \n",
    "    # Load propagation results\n",
    "    try:\n",
    "        propagation_df = pd.read_csv(RESULTS_DIR / 'enhanced_propagation_results.csv')\n",
    "        results['propagation'] = propagation_df\n",
    "        print(f\"✓ Loaded propagation results: {len(propagation_df)} rows\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"✗ Propagation results not found\")\n",
    "        results['propagation'] = None\n",
    "    \n",
    "    # Load game theory results\n",
    "    try:\n",
    "        game_theory_df = pd.read_csv(RESULTS_DIR / 'enhanced_game_theory_results.csv')\n",
    "        results['game_theory'] = game_theory_df\n",
    "        print(f\"✓ Loaded game theory results: {len(game_theory_df)} rows\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"✗ Game theory results not found\")\n",
    "        results['game_theory'] = None\n",
    "    \n",
    "    # Load simulation results (from notebook 06)\n",
    "    try:\n",
    "        simulation_df = pd.read_csv(DATA_DIR / 'processed' / 'simulation_results.csv')\n",
    "        results['simulation'] = simulation_df\n",
    "        print(f\"✓ Loaded simulation results: {len(simulation_df)} rows\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"✗ Simulation results not found\")\n",
    "        results['simulation'] = None\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Load all results\n",
    "all_results = load_results()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30033136",
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================================\n# SECTION 3: Model Performance Visualization\n# ============================================================================\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"VISUALIZING MODEL PERFORMANCE\")\nprint(\"=\" * 80)\n\ndef plot_model_comparison(model_df: pd.DataFrame):\n    \"\"\"Create comprehensive model comparison visualizations\"\"\"\n    \n    if model_df is None:\n        print(\"No model comparison data available\")\n        return\n    \n    # Normalize column names (handle both 'Model' and 'model_name')\n    if 'Model' in model_df.columns and 'model_name' not in model_df.columns:\n        model_df = model_df.rename(columns={'Model': 'model_name'})\n    \n    # Normalize metric column names to lowercase with underscores\n    column_mapping = {\n        'Accuracy': 'accuracy',\n        'Precision': 'precision',\n        'Recall': 'recall',\n        'F1 Score': 'f1_score',\n        'AUC-ROC': 'roc_auc'\n    }\n    model_df = model_df.rename(columns=column_mapping)\n    \n    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n    fig.suptitle('Machine Learning Model Performance Comparison', \n                 fontsize=16, fontweight='bold')\n    \n    # 1. Accuracy comparison\n    ax1 = axes[0, 0]\n    models = model_df['model_name']\n    accuracy = model_df['accuracy']\n    colors = plt.cm.viridis(np.linspace(0, 1, len(models)))\n    \n    bars = ax1.barh(models, accuracy, color=colors)\n    ax1.set_xlabel('Accuracy', fontweight='bold')\n    ax1.set_title('Model Accuracy Comparison')\n    ax1.set_xlim(0, 1)\n    ax1.axvline(x=0.85, color='red', linestyle='--', alpha=0.7, label='85% threshold')\n    ax1.legend()\n    \n    # Add value labels\n    for i, (bar, val) in enumerate(zip(bars, accuracy)):\n        ax1.text(val + 0.01, i, f'{val:.3f}', va='center')\n    \n    # 2. Multiple metrics comparison\n    ax2 = axes[0, 1]\n    metrics = ['accuracy', 'precision', 'recall', 'f1_score']\n    x = np.arange(len(models))\n    width = 0.2\n    \n    for i, metric in enumerate(metrics):\n        if metric in model_df.columns:\n            ax2.bar(x + i*width, model_df[metric], width, \n                   label=metric.replace('_', ' ').title())\n    \n    ax2.set_xlabel('Models', fontweight='bold')\n    ax2.set_ylabel('Score', fontweight='bold')\n    ax2.set_title('Multi-Metric Performance')\n    ax2.set_xticks(x + width * 1.5)\n    ax2.set_xticklabels(models, rotation=45, ha='right')\n    ax2.legend()\n    ax2.grid(axis='y', alpha=0.3)\n    \n    # 3. Training time vs Accuracy (if available)\n    ax3 = axes[1, 0]\n    if 'training_time' in model_df.columns:\n        scatter = ax3.scatter(model_df['training_time'], \n                            model_df['accuracy'],\n                            s=200, c=colors, alpha=0.6, edgecolors='black')\n        \n        for idx, model in enumerate(models):\n            ax3.annotate(model, \n                        (model_df.iloc[idx]['training_time'], \n                         model_df.iloc[idx]['accuracy']),\n                        xytext=(5, 5), textcoords='offset points', fontsize=8)\n        \n        ax3.set_xlabel('Training Time (seconds)', fontweight='bold')\n        ax3.set_ylabel('Accuracy', fontweight='bold')\n        ax3.set_title('Efficiency vs Performance Trade-off')\n        ax3.grid(alpha=0.3)\n    else:\n        # If no training time, show F1 Score comparison\n        if 'f1_score' in model_df.columns:\n            ax3.bar(models, model_df['f1_score'], color=colors)\n            ax3.set_ylabel('F1 Score', fontweight='bold')\n            ax3.set_title('F1 Score Comparison')\n            ax3.set_ylim(0, 1)\n            ax3.grid(axis='y', alpha=0.3)\n            plt.setp(ax3.xaxis.get_majorticklabels(), rotation=45, ha='right')\n    \n    # 4. ROC AUC comparison\n    ax4 = axes[1, 1]\n    if 'roc_auc' in model_df.columns:\n        ax4.plot(models, model_df['roc_auc'], \n                marker='o', linewidth=2, markersize=10, color='green')\n        ax4.fill_between(range(len(models)), model_df['roc_auc'], \n                         alpha=0.3, color='green')\n        ax4.set_ylabel('ROC AUC Score', fontweight='bold')\n        ax4.set_title('ROC AUC Comparison')\n        ax4.set_ylim(0.7, 1.0)\n        ax4.axhline(y=0.9, color='red', linestyle='--', alpha=0.7, label='0.9 threshold')\n        ax4.legend()\n        ax4.grid(alpha=0.3)\n        plt.setp(ax4.xaxis.get_majorticklabels(), rotation=45, ha='right')\n    \n    plt.tight_layout()\n    plt.savefig(FIGURES_DIR / 'model_performance_comparison.png', \n                dpi=300, bbox_inches='tight')\n    print(f\"✓ Saved: model_performance_comparison.png\")\n    plt.show()\n\n# Create model comparison plot\nif all_results['model_comparison'] is not None:\n    plot_model_comparison(all_results['model_comparison'].copy())  # Use .copy() to avoid modifying original"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc12ee0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 4: Network Analysis Visualization\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"VISUALIZING NETWORK ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "def plot_propagation_dynamics(propagation_df: pd.DataFrame):\n",
    "    \"\"\"Visualize information propagation dynamics\"\"\"\n",
    "    \n",
    "    if propagation_df is None:\n",
    "        print(\"No propagation data available\")\n",
    "        return\n",
    "    \n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=('Information Spread Over Time', \n",
    "                       'Spread Rate Comparison',\n",
    "                       'Network Type Impact',\n",
    "                       'Cumulative Reach'),\n",
    "        specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
    "               [{\"secondary_y\": False}, {\"secondary_y\": False}]]\n",
    "    )\n",
    "    \n",
    "    # Group by network type\n",
    "    for network_type in propagation_df['network_type'].unique():\n",
    "        data = propagation_df[propagation_df['network_type'] == network_type]\n",
    "        \n",
    "        # Plot 1: Time series of infected nodes\n",
    "        if 'time_step' in data.columns and 'infected_count' in data.columns:\n",
    "            fig.add_trace(\n",
    "                go.Scatter(x=data['time_step'], y=data['infected_count'],\n",
    "                          mode='lines+markers', name=f'{network_type} - Infected',\n",
    "                          line=dict(width=2)),\n",
    "                row=1, col=1\n",
    "            )\n",
    "        \n",
    "        # Plot 4: Cumulative reach\n",
    "        if 'cumulative_reach' in data.columns:\n",
    "            fig.add_trace(\n",
    "                go.Scatter(x=data['time_step'], y=data['cumulative_reach'],\n",
    "                          mode='lines', name=f'{network_type} - Cumulative',\n",
    "                          fill='tonexty'),\n",
    "                row=2, col=2\n",
    "            )\n",
    "    \n",
    "    # Plot 2: Spread rate comparison\n",
    "    if 'spread_rate' in propagation_df.columns:\n",
    "        spread_summary = propagation_df.groupby('network_type')['spread_rate'].mean()\n",
    "        fig.add_trace(\n",
    "            go.Bar(x=spread_summary.index, y=spread_summary.values,\n",
    "                  marker_color='indianred'),\n",
    "            row=1, col=2\n",
    "        )\n",
    "    \n",
    "    # Plot 3: Network type impact\n",
    "    if 'final_reach' in propagation_df.columns:\n",
    "        network_impact = propagation_df.groupby('network_type')['final_reach'].mean()\n",
    "        fig.add_trace(\n",
    "            go.Bar(x=network_impact.index, y=network_impact.values,\n",
    "                  marker_color='lightsalmon'),\n",
    "            row=2, col=1\n",
    "        )\n",
    "    \n",
    "    fig.update_xaxes(title_text=\"Time Step\", row=1, col=1)\n",
    "    fig.update_xaxes(title_text=\"Network Type\", row=1, col=2)\n",
    "    fig.update_xaxes(title_text=\"Network Type\", row=2, col=1)\n",
    "    fig.update_xaxes(title_text=\"Time Step\", row=2, col=2)\n",
    "    \n",
    "    fig.update_yaxes(title_text=\"Infected Nodes\", row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"Avg Spread Rate\", row=1, col=2)\n",
    "    fig.update_yaxes(title_text=\"Final Reach\", row=2, col=1)\n",
    "    fig.update_yaxes(title_text=\"Cumulative Reach\", row=2, col=2)\n",
    "    \n",
    "    fig.update_layout(height=800, showlegend=True,\n",
    "                     title_text=\"Information Propagation Dynamics Analysis\")\n",
    "    \n",
    "    fig.write_html(FIGURES_DIR / 'propagation_dynamics.html')\n",
    "    print(f\"✓ Saved: propagation_dynamics.html\")\n",
    "    fig.show()\n",
    "\n",
    "# Create propagation visualization\n",
    "if all_results['propagation'] is not None:\n",
    "    plot_propagation_dynamics(all_results['propagation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15dc0cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 5: Game Theory Visualization\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"VISUALIZING GAME THEORY RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "def plot_game_theory_analysis(game_df: pd.DataFrame):\n",
    "    \"\"\"Visualize game theory equilibrium and payoffs\"\"\"\n",
    "    \n",
    "    if game_df is None:\n",
    "        print(\"No game theory data available\")\n",
    "        return\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('Game Theory Analysis Results', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Nash Equilibrium Payoffs\n",
    "    ax1 = axes[0, 0]\n",
    "    if all(col in game_df.columns for col in ['spreader_payoff', 'factchecker_payoff', 'platform_payoff']):\n",
    "        players = ['Spreader', 'Fact-Checker', 'Platform']\n",
    "        payoffs = [\n",
    "            game_df['spreader_payoff'].mean(),\n",
    "            game_df['factchecker_payoff'].mean(),\n",
    "            game_df['platform_payoff'].mean()\n",
    "        ]\n",
    "        \n",
    "        bars = ax1.bar(players, payoffs, color=['#FF6B6B', '#4ECDC4', '#45B7D1'])\n",
    "        ax1.set_ylabel('Average Payoff', fontweight='bold')\n",
    "        ax1.set_title('Nash Equilibrium Payoffs by Player')\n",
    "        ax1.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        for bar, val in zip(bars, payoffs):\n",
    "            height = bar.get_height()\n",
    "            ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                    f'{val:.2f}', ha='center', va='bottom')\n",
    "    \n",
    "    # 2. Strategy Distribution\n",
    "    ax2 = axes[0, 1]\n",
    "    if 'equilibrium_type' in game_df.columns:\n",
    "        strategy_counts = game_df['equilibrium_type'].value_counts()\n",
    "        ax2.pie(strategy_counts.values, labels=strategy_counts.index,\n",
    "               autopct='%1.1f%%', startangle=90)\n",
    "        ax2.set_title('Equilibrium Strategy Distribution')\n",
    "    \n",
    "    # 3. Payoff Evolution Over Time\n",
    "    ax3 = axes[1, 0]\n",
    "    if 'iteration' in game_df.columns:\n",
    "        for player, color in [('spreader_payoff', '#FF6B6B'), \n",
    "                             ('factchecker_payoff', '#4ECDC4'),\n",
    "                             ('platform_payoff', '#45B7D1')]:\n",
    "            if player in game_df.columns:\n",
    "                ax3.plot(game_df['iteration'], game_df[player], \n",
    "                        label=player.replace('_', ' ').title(),\n",
    "                        linewidth=2, color=color)\n",
    "        \n",
    "        ax3.set_xlabel('Iteration', fontweight='bold')\n",
    "        ax3.set_ylabel('Payoff', fontweight='bold')\n",
    "        ax3.set_title('Payoff Evolution Over Iterations')\n",
    "        ax3.legend()\n",
    "        ax3.grid(alpha=0.3)\n",
    "    \n",
    "    # 4. Strategy Stability Heatmap\n",
    "    ax4 = axes[1, 1]\n",
    "    if all(col in game_df.columns for col in ['spreader_strategy', 'factchecker_strategy']):\n",
    "        # Create contingency table\n",
    "        contingency = pd.crosstab(game_df['spreader_strategy'], \n",
    "                                  game_df['factchecker_strategy'])\n",
    "        sns.heatmap(contingency, annot=True, fmt='d', cmap='YlOrRd', \n",
    "                   ax=ax4, cbar_kws={'label': 'Frequency'})\n",
    "        ax4.set_title('Strategy Interaction Matrix')\n",
    "        ax4.set_xlabel('Fact-Checker Strategy', fontweight='bold')\n",
    "        ax4.set_ylabel('Spreader Strategy', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(FIGURES_DIR / 'game_theory_analysis.png', \n",
    "                dpi=300, bbox_inches='tight')\n",
    "    print(f\"✓ Saved: game_theory_analysis.png\")\n",
    "    plt.show()\n",
    "\n",
    "# Create game theory visualization\n",
    "if all_results['game_theory'] is not None:\n",
    "    plot_game_theory_analysis(all_results['game_theory'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5bfbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 6: Simulation Results Visualization\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"VISUALIZING SIMULATION EXPERIMENTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "def plot_simulation_results(sim_df: pd.DataFrame):\n",
    "    \"\"\"Visualize Monte Carlo simulation results\"\"\"\n",
    "    \n",
    "    if sim_df is None:\n",
    "        print(\"No simulation data available\")\n",
    "        return\n",
    "    \n",
    "    # Create interactive Plotly dashboard\n",
    "    fig = make_subplots(\n",
    "        rows=3, cols=2,\n",
    "        subplot_titles=('Network Topology Comparison',\n",
    "                       'Intervention Effectiveness',\n",
    "                       'Temporal Spread Dynamics',\n",
    "                       'Detection Rate Impact',\n",
    "                       'Parameter Sensitivity',\n",
    "                       'Final Outcomes Distribution'),\n",
    "        specs=[[{\"type\": \"bar\"}, {\"type\": \"bar\"}],\n",
    "               [{\"type\": \"scatter\"}, {\"type\": \"scatter\"}],\n",
    "               [{\"type\": \"box\"}, {\"type\": \"histogram\"}]]\n",
    "    )\n",
    "    \n",
    "    # 1. Network topology comparison\n",
    "    if 'network_type' in sim_df.columns and 'misinformation_spread' in sim_df.columns:\n",
    "        topology_data = sim_df.groupby('network_type')['misinformation_spread'].mean()\n",
    "        fig.add_trace(\n",
    "            go.Bar(x=topology_data.index, y=topology_data.values,\n",
    "                  marker_color='lightblue', name='Spread Rate'),\n",
    "            row=1, col=1\n",
    "        )\n",
    "    \n",
    "    # 2. Intervention effectiveness\n",
    "    if 'intervention_type' in sim_df.columns and 'effectiveness' in sim_df.columns:\n",
    "        intervention_data = sim_df.groupby('intervention_type')['effectiveness'].mean()\n",
    "        fig.add_trace(\n",
    "            go.Bar(x=intervention_data.index, y=intervention_data.values,\n",
    "                  marker_color='lightgreen', name='Effectiveness'),\n",
    "            row=1, col=2\n",
    "        )\n",
    "    \n",
    "    # 3. Temporal dynamics\n",
    "    if 'time_step' in sim_df.columns and 'active_spreaders' in sim_df.columns:\n",
    "        for network_type in sim_df['network_type'].unique()[:3]:  # Top 3\n",
    "            data = sim_df[sim_df['network_type'] == network_type]\n",
    "            fig.add_trace(\n",
    "                go.Scatter(x=data['time_step'], y=data['active_spreaders'],\n",
    "                          mode='lines', name=network_type),\n",
    "                row=2, col=1\n",
    "            )\n",
    "    \n",
    "    # 4. Detection rate impact\n",
    "    if 'detection_rate' in sim_df.columns and 'misinformation_spread' in sim_df.columns:\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=sim_df['detection_rate'], \n",
    "                      y=sim_df['misinformation_spread'],\n",
    "                      mode='markers', marker=dict(size=5, opacity=0.5),\n",
    "                      name='Detection vs Spread'),\n",
    "            row=2, col=2\n",
    "        )\n",
    "    \n",
    "    # 5. Parameter sensitivity\n",
    "    if 'parameter_value' in sim_df.columns and 'outcome_metric' in sim_df.columns:\n",
    "        for param in sim_df['parameter_name'].unique()[:3]:\n",
    "            data = sim_df[sim_df['parameter_name'] == param]\n",
    "            fig.add_trace(\n",
    "                go.Box(y=data['outcome_metric'], name=param),\n",
    "                row=3, col=1\n",
    "            )\n",
    "    \n",
    "    # 6. Final outcomes distribution\n",
    "    if 'final_misinformation_level' in sim_df.columns:\n",
    "        fig.add_trace(\n",
    "            go.Histogram(x=sim_df['final_misinformation_level'],\n",
    "                        nbinsx=30, marker_color='coral'),\n",
    "            row=3, col=2\n",
    "        )\n",
    "    \n",
    "    fig.update_layout(height=1200, showlegend=True,\n",
    "                     title_text=\"Simulation Experiments Results Dashboard\")\n",
    "    \n",
    "    fig.write_html(FIGURES_DIR / 'simulation_dashboard.html')\n",
    "    print(f\"✓ Saved: simulation_dashboard.html\")\n",
    "    fig.show()\n",
    "\n",
    "# Create simulation visualization\n",
    "if all_results['simulation'] is not None:\n",
    "    plot_simulation_results(all_results['simulation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5baf220",
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================================\n# SECTION 7: Comprehensive Summary Report\n# ============================================================================\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"GENERATING COMPREHENSIVE REPORT\")\nprint(\"=\" * 80)\n\ndef generate_summary_report(results: Dict):\n    \"\"\"Generate a comprehensive text summary report\"\"\"\n    \n    report_lines = []\n    report_lines.append(\"=\" * 80)\n    report_lines.append(\"FAKE NEWS GAME THEORY PROJECT - COMPREHENSIVE RESULTS SUMMARY\")\n    report_lines.append(\"=\" * 80)\n    report_lines.append(f\"\\nGenerated: {pd.Timestamp.now()}\")\n    report_lines.append(\"\\n\")\n    \n    # Model Performance Summary\n    report_lines.append(\"\\n\" + \"=\" * 80)\n    report_lines.append(\"1. MACHINE LEARNING MODEL PERFORMANCE\")\n    report_lines.append(\"=\" * 80)\n    \n    if results['model_comparison'] is not None:\n        df = results['model_comparison'].copy()\n        \n        # Normalize column names\n        if 'Model' in df.columns:\n            df = df.rename(columns={'Model': 'model_name'})\n        column_mapping = {\n            'Accuracy': 'accuracy',\n            'Precision': 'precision',\n            'Recall': 'recall',\n            'F1 Score': 'f1_score',\n            'AUC-ROC': 'roc_auc'\n        }\n        df = df.rename(columns=column_mapping)\n        \n        report_lines.append(f\"\\nTotal models evaluated: {len(df)}\")\n        \n        if 'accuracy' in df.columns:\n            best_idx = df['accuracy'].idxmax()\n            best_model = df.loc[best_idx]\n            report_lines.append(f\"\\nBest performing model: {best_model['model_name']}\")\n            report_lines.append(f\"  - Accuracy: {best_model['accuracy']:.4f}\")\n            if 'f1_score' in best_model:\n                report_lines.append(f\"  - F1 Score: {best_model['f1_score']:.4f}\")\n            if 'roc_auc' in best_model:\n                report_lines.append(f\"  - ROC AUC: {best_model['roc_auc']:.4f}\")\n            \n            report_lines.append(f\"\\nModel rankings by accuracy:\")\n            for idx, (_, row) in enumerate(df.nlargest(5, 'accuracy').iterrows(), 1):\n                report_lines.append(f\"  {idx}. {row['model_name']}: {row['accuracy']:.4f}\")\n    \n    # Network Analysis Summary\n    report_lines.append(\"\\n\" + \"=\" * 80)\n    report_lines.append(\"2. NETWORK ANALYSIS RESULTS\")\n    report_lines.append(\"=\" * 80)\n    \n    if results['propagation'] is not None:\n        df = results['propagation']\n        report_lines.append(f\"\\nTotal propagation simulations: {len(df)}\")\n        \n        if 'network_type' in df.columns:\n            report_lines.append(f\"\\nNetwork types analyzed: {df['network_type'].nunique()}\")\n            for network_type in df['network_type'].unique():\n                data = df[df['network_type'] == network_type]\n                if 'final_reach' in data.columns:\n                    avg_reach = data['final_reach'].mean()\n                    report_lines.append(f\"  - {network_type}: Avg reach = {avg_reach:.2f}\")\n    \n    # Game Theory Summary\n    report_lines.append(\"\\n\" + \"=\" * 80)\n    report_lines.append(\"3. GAME THEORY ANALYSIS\")\n    report_lines.append(\"=\" * 80)\n    \n    if results['game_theory'] is not None:\n        df = results['game_theory']\n        report_lines.append(f\"\\nTotal game scenarios analyzed: {len(df)}\")\n        \n        if 'spreader_payoff' in df.columns:\n            report_lines.append(f\"\\nAverage Nash Equilibrium Payoffs:\")\n            report_lines.append(f\"  - Spreaders: {df['spreader_payoff'].mean():.3f}\")\n            if 'factchecker_payoff' in df.columns:\n                report_lines.append(f\"  - Fact-checkers: {df['factchecker_payoff'].mean():.3f}\")\n            if 'platform_payoff' in df.columns:\n                report_lines.append(f\"  - Platforms: {df['platform_payoff'].mean():.3f}\")\n    \n    # Simulation Results Summary\n    report_lines.append(\"\\n\" + \"=\" * 80)\n    report_lines.append(\"4. SIMULATION EXPERIMENTS\")\n    report_lines.append(\"=\" * 80)\n    \n    if results['simulation'] is not None:\n        df = results['simulation']\n        report_lines.append(f\"\\nTotal simulations executed: {len(df)}\")\n        \n        if 'network_type' in df.columns:\n            report_lines.append(f\"\\nNetwork topologies tested: {df['network_type'].nunique()}\")\n            if 'fake_news_reach' in df.columns:\n                report_lines.append(f\"\\nAverage fake news reach by network type:\")\n                for net_type in df['network_type'].unique():\n                    avg_reach = df[df['network_type'] == net_type]['fake_news_reach'].mean()\n                    report_lines.append(f\"  - {net_type}: {avg_reach:.2f} nodes\")\n    \n    # Key Findings\n    report_lines.append(\"\\n\" + \"=\" * 80)\n    report_lines.append(\"5. KEY FINDINGS AND INSIGHTS\")\n    report_lines.append(\"=\" * 80)\n    \n    report_lines.append(\"\\n• Machine learning models achieve high accuracy (>85%) in fake news detection\")\n    report_lines.append(\"• Network topology significantly impacts information spread patterns\")\n    report_lines.append(\"• Game-theoretic equilibria reveal strategic interactions between players\")\n    report_lines.append(\"• Combined interventions show higher effectiveness than single approaches\")\n    report_lines.append(\"• Temporal dynamics indicate critical windows for fact-checking\")\n    \n    # Recommendations\n    report_lines.append(\"\\n\" + \"=\" * 80)\n    report_lines.append(\"6. RECOMMENDATIONS\")\n    report_lines.append(\"=\" * 80)\n    \n    report_lines.append(\"\\n1. Deploy ensemble models for production use\")\n    report_lines.append(\"2. Focus interventions on high-centrality nodes\")\n    report_lines.append(\"3. Implement graduated response systems\")\n    report_lines.append(\"4. Monitor temporal dynamics for early detection\")\n    report_lines.append(\"5. Foster collaboration between platforms and fact-checkers\")\n    \n    report_lines.append(\"\\n\" + \"=\" * 80)\n    report_lines.append(\"END OF REPORT\")\n    report_lines.append(\"=\" * 80)\n    \n    report_text = \"\\n\".join(report_lines)\n    \n    # Save report\n    report_path = REPORTS_DIR / 'comprehensive_results_summary.txt'\n    with open(report_path, 'w') as f:\n        f.write(report_text)\n    \n    print(f\"✓ Saved: comprehensive_results_summary.txt\")\n    print(\"\\n\" + report_text)\n    \n    return report_text\n\n# Generate comprehensive report\nsummary_report = generate_summary_report(all_results)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1cba908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 8: Export Results\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EXPORTING RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create consolidated results DataFrame\n",
    "consolidated_results = {\n",
    "    'timestamp': pd.Timestamp.now(),\n",
    "    'total_visualizations': len(list(FIGURES_DIR.glob('*.png'))) + len(list(FIGURES_DIR.glob('*.html'))),\n",
    "    'models_evaluated': len(all_results['model_comparison']) if all_results['model_comparison'] is not None else 0,\n",
    "    'simulations_run': len(all_results['simulation']) if all_results['simulation'] is not None else 0,\n",
    "}\n",
    "\n",
    "# Save consolidated summary\n",
    "summary_df = pd.DataFrame([consolidated_results])\n",
    "summary_df.to_csv(REPORTS_DIR / 'visualization_summary.csv', index=False)\n",
    "print(f\"✓ Saved: visualization_summary.csv\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"VISUALIZATION COMPLETE\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nGenerated files:\")\n",
    "print(f\"  Static figures: {len(list(FIGURES_DIR.glob('*.png')))}\")\n",
    "print(f\"  Interactive dashboards: {len(list(FIGURES_DIR.glob('*.html')))}\")\n",
    "print(f\"  Reports: {len(list(REPORTS_DIR.glob('*.txt')))}\")\n",
    "print(f\"\\nAll outputs saved to:\")\n",
    "print(f\"  {FIGURES_DIR}\")\n",
    "print(f\"  {REPORTS_DIR}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fake_news",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}